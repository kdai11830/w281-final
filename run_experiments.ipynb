{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import cv2\n",
    "from skimage import feature, exposure\n",
    "import colorsys\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torchvision.models as models\n",
    "from torchvision.models import resnet101, ResNet101_Weights\n",
    "from torchvision.models import vgg16, VGG16_Weights\n",
    "from torchvision.models import efficientnet_v2_m, EfficientNet_V2_M_Weights\n",
    "from torchvision import transforms\n",
    "from pytorch_model_summary import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from HOG_and_DAISY_feature_extraction_methods import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data into Train (and Validation) and a Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load all 30 classes, 220 images per class\n",
    "# X, Y, idx_to_cl = load_dataset('data', cl_limit=30, img_limit=220)\n",
    "\n",
    "# # train test split at 80-20 ratio\n",
    "# X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, stratify=Y, shuffle=True, random_state=42)\n",
    "\n",
    "# print(f\"Data shape: {X.shape}\")\n",
    "# print(f\"Data labels shape: {Y.shape}\\n\")\n",
    "# print(f\"X Train shape: {X_train.shape}\")\n",
    "# print(f\"Y Train shape: {Y_train.shape}\")\n",
    "# print(f\"X Test shape: {X_test.shape}\")\n",
    "# print(f\"Y Test shape: {Y_test.shape}\")\n",
    "\n",
    "# # save all as np\n",
    "# folder_path = Path('train_test_split')\n",
    "# folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# with open(folder_path / 'X_train.npy', 'wb') as f:\n",
    "#     np.save(f, X_train)\n",
    "# with open(folder_path / 'X_test.npy', 'wb') as f:\n",
    "#     np.save(f, X_test)\n",
    "# with open(folder_path / 'Y_train.npy', 'wb') as f:\n",
    "#     np.save(f, Y_train)\n",
    "# with open(folder_path / 'Y_test.npy', 'wb') as f:\n",
    "#     np.save(f, Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Train shape: (5280, 600, 600, 3)\n",
      "Y Train shape: (5280,)\n",
      "X Test shape: (1320, 600, 600, 3)\n",
      "Y Test shape: (1320,)\n"
     ]
    }
   ],
   "source": [
    "# load in train and test data\n",
    "folder_path = Path('train_test_split')\n",
    "\n",
    "X_train = np.load(folder_path / 'X_train.npy')\n",
    "Y_train = np.load(folder_path / 'Y_train.npy')\n",
    "X_test = np.load(folder_path / 'X_test.npy')\n",
    "Y_test = np.load(folder_path / 'Y_test.npy')\n",
    "\n",
    "print(f\"X Train shape: {X_train.shape}\")\n",
    "print(f\"Y Train shape: {Y_train.shape}\")\n",
    "print(f\"X Test shape: {X_test.shape}\")\n",
    "print(f\"Y Test shape: {Y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_features():\n",
    "    \"\"\" \"\"\"\n",
    "    features, features_idxs = apply_features(X=X_train, feature_functions = {\n",
    "        'temp': extract_small_brightness_hog_features,\n",
    "    })\n",
    "    \n",
    "    # save features\n",
    "    feature_folder_path = Path('features')\n",
    "\n",
    "    with open(feature_folder_path / 'train_small_brightness_HOG_features.npy', 'wb') as f:\n",
    "        np.save(f, features)\n",
    "        \n",
    "# process_features()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    " - PCA variance explained\n",
    "    - load in all features \n",
    "    - plot variance explained together\n",
    " - Pick a final feature set\n",
    "    - do PCA\n",
    "    - do t-SNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features():\n",
    "    \"\"\" \"\"\"\n",
    "    feature_dict = {}\n",
    "    features_dir = Path('features')\n",
    "    for path in Path('features').iterdir():\n",
    "        feature_name = path.stem\n",
    "        feature_data = np.load(path)\n",
    "        feature_dict[feature_name] = feature_data\n",
    "    return feature_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dict = load_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # features scaling\n",
    "# scaler = StandardScaler()\n",
    "# features_train_scaled = scaler.fit_transform(multiple_features) # replace input with whatever experiments we are doing\n",
    "# # TODO: save this scaler for final features only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # PCA for features, use 95% explained variance for now\n",
    "# pca = PCA(n_components=0.95)\n",
    "# train_transformed = pca.fit_transform(features_train_scaled)\n",
    "# print(train_transformed.shape)\n",
    "\n",
    "# # TODO: save this pca model for final features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan\n",
    " - iterate throught the feature_dict\n",
    "    - for each feature set, train a Log Reg model\n",
    "    - report accuracy, f1-weighted and time info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_models(feature_dictionary, Y_train):\n",
    "    \"\"\" \"\"\"\n",
    "    results_dict = {}\n",
    "    for feature_name, feature_data in tqdm(feature_dictionary.items()):\n",
    "        lr = LogisticRegression(\n",
    "            penalty= \"l2\",\n",
    "            tol = 1e-4,\n",
    "            C=1.0,\n",
    "            solver=\"lbfgs\",\n",
    "            max_iter=150,\n",
    "            multi_class=\"multinomial\",\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "\n",
    "        # replace experiment with whatever feature we are testing\n",
    "        lr_model = cross_validate(lr, feature_data, Y_train, scoring=('f1_weighted','accuracy'), cv=10, n_jobs=-1)\n",
    "        \n",
    "        results_dict[feature_name] = lr_model\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 22/22 [30:20<00:00, 82.74s/it] \n"
     ]
    }
   ],
   "source": [
    "results_dict = try_models(feature_dict, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names_list = []\n",
    "fit_time_list = []\n",
    "score_time_list = []\n",
    "f1_list = []\n",
    "accuracy_list = []\n",
    "\n",
    "for feature_name, info_dict in results_dict.items():\n",
    "   \n",
    "    feature_names_list.append(feature_name)\n",
    "    fit_time_list.append(np.mean(info_dict['fit_time']))\n",
    "    score_time_list.append(np.mean(info_dict['score_time']))\n",
    "    f1_list.append(np.mean(info_dict['test_f1_weighted']))\n",
    "    accuracy_list.append(np.mean(info_dict['test_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature_Name</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1_Score</th>\n",
       "      <th>Train_Time</th>\n",
       "      <th>Evaluate_Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>train_ResNet_features</td>\n",
       "      <td>0.917235</td>\n",
       "      <td>0.917039</td>\n",
       "      <td>54.139675</td>\n",
       "      <td>0.015045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train_EffNet_features</td>\n",
       "      <td>0.907576</td>\n",
       "      <td>0.907905</td>\n",
       "      <td>33.202282</td>\n",
       "      <td>0.008705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>train_small_hue_HOG_features</td>\n",
       "      <td>0.310985</td>\n",
       "      <td>0.305891</td>\n",
       "      <td>217.378017</td>\n",
       "      <td>0.037060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>train_small_saturation_HOG_features</td>\n",
       "      <td>0.309659</td>\n",
       "      <td>0.296947</td>\n",
       "      <td>208.478318</td>\n",
       "      <td>0.043910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>train_large_hue_HOG_features</td>\n",
       "      <td>0.302652</td>\n",
       "      <td>0.296977</td>\n",
       "      <td>23.463807</td>\n",
       "      <td>0.006006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>train_small_green_HOG_features</td>\n",
       "      <td>0.302083</td>\n",
       "      <td>0.287884</td>\n",
       "      <td>220.557816</td>\n",
       "      <td>0.041026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>train_small_gray_HOG_features</td>\n",
       "      <td>0.298864</td>\n",
       "      <td>0.286060</td>\n",
       "      <td>213.320857</td>\n",
       "      <td>0.046007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>train_small_brightness_HOG_features</td>\n",
       "      <td>0.298106</td>\n",
       "      <td>0.283539</td>\n",
       "      <td>215.857707</td>\n",
       "      <td>0.046792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>train_small_red_HOG_features</td>\n",
       "      <td>0.296970</td>\n",
       "      <td>0.284441</td>\n",
       "      <td>215.376529</td>\n",
       "      <td>0.048306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>train_small_blue_HOG_features</td>\n",
       "      <td>0.295076</td>\n",
       "      <td>0.279959</td>\n",
       "      <td>219.546621</td>\n",
       "      <td>0.043214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>train_large_gray_HOG_features</td>\n",
       "      <td>0.278030</td>\n",
       "      <td>0.266207</td>\n",
       "      <td>23.944475</td>\n",
       "      <td>0.005702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>train_large_brightness_HOG_features</td>\n",
       "      <td>0.271212</td>\n",
       "      <td>0.258944</td>\n",
       "      <td>25.163568</td>\n",
       "      <td>0.006602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>train_large_red_HOG_features</td>\n",
       "      <td>0.270833</td>\n",
       "      <td>0.256208</td>\n",
       "      <td>24.150263</td>\n",
       "      <td>0.007054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>train_large_green_HOG_features</td>\n",
       "      <td>0.270455</td>\n",
       "      <td>0.259619</td>\n",
       "      <td>24.037988</td>\n",
       "      <td>0.007040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>train_large_blue_HOG_features</td>\n",
       "      <td>0.265909</td>\n",
       "      <td>0.253130</td>\n",
       "      <td>24.211721</td>\n",
       "      <td>0.006555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>train_HSV_features</td>\n",
       "      <td>0.255682</td>\n",
       "      <td>0.228360</td>\n",
       "      <td>0.860492</td>\n",
       "      <td>0.002906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>train_large_saturation_HOG_features</td>\n",
       "      <td>0.251894</td>\n",
       "      <td>0.234666</td>\n",
       "      <td>27.307444</td>\n",
       "      <td>0.006713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>train_RGB_features</td>\n",
       "      <td>0.209659</td>\n",
       "      <td>0.181992</td>\n",
       "      <td>0.979477</td>\n",
       "      <td>0.003009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train_daisy_gray_features</td>\n",
       "      <td>0.152083</td>\n",
       "      <td>0.104778</td>\n",
       "      <td>4.447340</td>\n",
       "      <td>0.015155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train_daisy_red_features</td>\n",
       "      <td>0.151136</td>\n",
       "      <td>0.103511</td>\n",
       "      <td>5.062865</td>\n",
       "      <td>0.010302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train_daisy_blue_features</td>\n",
       "      <td>0.150758</td>\n",
       "      <td>0.100702</td>\n",
       "      <td>4.772220</td>\n",
       "      <td>0.013658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train_daisy_green_features</td>\n",
       "      <td>0.149811</td>\n",
       "      <td>0.102191</td>\n",
       "      <td>4.946945</td>\n",
       "      <td>0.009660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Feature_Name  Accuracy  F1_Score  Train_Time  \\\n",
       "13                train_ResNet_features  0.917235  0.917039   54.139675   \n",
       "4                 train_EffNet_features  0.907576  0.907905   33.202282   \n",
       "19         train_small_hue_HOG_features  0.310985  0.305891  217.378017   \n",
       "21  train_small_saturation_HOG_features  0.309659  0.296947  208.478318   \n",
       "10         train_large_hue_HOG_features  0.302652  0.296977   23.463807   \n",
       "18       train_small_green_HOG_features  0.302083  0.287884  220.557816   \n",
       "17        train_small_gray_HOG_features  0.298864  0.286060  213.320857   \n",
       "16  train_small_brightness_HOG_features  0.298106  0.283539  215.857707   \n",
       "20         train_small_red_HOG_features  0.296970  0.284441  215.376529   \n",
       "15        train_small_blue_HOG_features  0.295076  0.279959  219.546621   \n",
       "8         train_large_gray_HOG_features  0.278030  0.266207   23.944475   \n",
       "7   train_large_brightness_HOG_features  0.271212  0.258944   25.163568   \n",
       "11         train_large_red_HOG_features  0.270833  0.256208   24.150263   \n",
       "9        train_large_green_HOG_features  0.270455  0.259619   24.037988   \n",
       "6         train_large_blue_HOG_features  0.265909  0.253130   24.211721   \n",
       "5                    train_HSV_features  0.255682  0.228360    0.860492   \n",
       "12  train_large_saturation_HOG_features  0.251894  0.234666   27.307444   \n",
       "14                   train_RGB_features  0.209659  0.181992    0.979477   \n",
       "1             train_daisy_gray_features  0.152083  0.104778    4.447340   \n",
       "3              train_daisy_red_features  0.151136  0.103511    5.062865   \n",
       "0             train_daisy_blue_features  0.150758  0.100702    4.772220   \n",
       "2            train_daisy_green_features  0.149811  0.102191    4.946945   \n",
       "\n",
       "    Evaluate_Time  \n",
       "13       0.015045  \n",
       "4        0.008705  \n",
       "19       0.037060  \n",
       "21       0.043910  \n",
       "10       0.006006  \n",
       "18       0.041026  \n",
       "17       0.046007  \n",
       "16       0.046792  \n",
       "20       0.048306  \n",
       "15       0.043214  \n",
       "8        0.005702  \n",
       "7        0.006602  \n",
       "11       0.007054  \n",
       "9        0.007040  \n",
       "6        0.006555  \n",
       "5        0.002906  \n",
       "12       0.006713  \n",
       "14       0.003009  \n",
       "1        0.015155  \n",
       "3        0.010302  \n",
       "0        0.013658  \n",
       "2        0.009660  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fill_dict = {\n",
    "    \"Feature_Name\": feature_names_list,\n",
    "    \"Accuracy\": accuracy_list,\n",
    "    \"F1_Score\": f1_list,\n",
    "    \"Train_Time\": fit_time_list,\n",
    "    \"Evaluate_Time\": score_time_list,\n",
    "}\n",
    "\n",
    "results_df = pd.DataFrame(fill_dict)\n",
    "results_df.sort_values(by=[\"Accuracy\"], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(\"Initial_Features_LogReg_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features to Join\n",
    " - RGB, HSV\n",
    " - HOG (Small, Red), HOG (Large, Red), HOG (Small, Green), HOG (Large, Green), HOG (Small, Blue), HOG (Large, Blue)\n",
    " - HOG (Small, Gray), HOG (Large, Gray), HOG (Small, Hue), HOG (Large, Hue), HOG (Small, Saturation), HOG (Large, Saturation), HOG (Small, Brightness), HOG (Large, Brightness)\n",
    " - DAISY (Red), DAISY (Green), DAISY (Blue)\n",
    " - RGB, HSV, DAISY (Gray)\n",
    " - RGB, HSV, DAISY (Gray), ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_daisy_blue_features\n",
      "train_daisy_gray_features\n",
      "train_daisy_green_features\n",
      "train_daisy_red_features\n",
      "train_EffNet_features\n",
      "train_HSV_features\n",
      "train_large_blue_HOG_features\n",
      "train_large_brightness_HOG_features\n",
      "train_large_gray_HOG_features\n",
      "train_large_green_HOG_features\n",
      "train_large_hue_HOG_features\n",
      "train_large_red_HOG_features\n",
      "train_large_saturation_HOG_features\n",
      "train_ResNet_features\n",
      "train_RGB_features\n",
      "train_small_blue_HOG_features\n",
      "train_small_brightness_HOG_features\n",
      "train_small_gray_HOG_features\n",
      "train_small_green_HOG_features\n",
      "train_small_hue_HOG_features\n",
      "train_small_red_HOG_features\n",
      "train_small_saturation_HOG_features\n"
     ]
    }
   ],
   "source": [
    "for name in feature_dict.keys():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 5400)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RGB_DAISY_features = np.hstack((feature_dict['train_daisy_red_features'], feature_dict['train_daisy_green_features'], feature_dict['train_daisy_blue_features']))\n",
    "RGB_DAISY_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9000)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_hog_features = np.hstack((feature_dict['train_small_red_HOG_features'], feature_dict['train_large_red_HOG_features']))\n",
    "red_hog_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9000)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "green_hog_features = np.hstack((feature_dict['train_small_green_HOG_features'], feature_dict['train_large_green_HOG_features']))\n",
    "green_hog_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9000)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blue_hog_features = np.hstack((feature_dict['train_small_blue_HOG_features'], feature_dict['train_large_blue_HOG_features']))\n",
    "blue_hog_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9000)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gray_hog_features = np.hstack((feature_dict['train_small_gray_HOG_features'], feature_dict['train_large_gray_HOG_features']))\n",
    "gray_hog_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9000)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hue_hog_features = np.hstack((feature_dict['train_small_hue_HOG_features'], feature_dict['train_large_hue_HOG_features']))\n",
    "hue_hog_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9000)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saturation_hog_features = np.hstack((feature_dict['train_small_saturation_HOG_features'], feature_dict['train_large_saturation_HOG_features']))\n",
    "saturation_hog_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 9000)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brightness_hog_features = np.hstack((feature_dict['train_small_brightness_HOG_features'], feature_dict['train_large_brightness_HOG_features']))\n",
    "brightness_hog_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 12)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RGB_HSV_features = np.hstack((feature_dict['train_RGB_features'], feature_dict['train_HSV_features']))\n",
    "RGB_HSV_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 27000)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combo hog RGB\n",
    "hog_RGB_features = np.hstack((red_hog_features, green_hog_features, blue_hog_features))\n",
    "hog_RGB_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 36000)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combo hog gray + HSV\n",
    "hog_gray_HSV_features = np.hstack((gray_hog_features, hue_hog_features, saturation_hog_features, brightness_hog_features))\n",
    "hog_gray_HSV_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 2060)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RGB, HSV, ResNet\n",
    "RGB_HSV_ResNet_features = np.hstack((RGB_HSV_features, feature_dict['train_ResNet_features']))\n",
    "RGB_HSV_ResNet_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5280, 11060)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# RGB, HSV, ResNet, Gray HOG\n",
    "RGB_HSV_ResNet_GrayHOG_features = np.hstack((RGB_HSV_ResNet_features, gray_hog_features))\n",
    "RGB_HSV_ResNet_GrayHOG_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_combos_dict = {\n",
    "    \"DAISY (RGB)\": RGB_DAISY_features,\n",
    "    \"HOG Multi-Scale (Red)\": red_hog_features,\n",
    "    \"HOG Multi-Scale (Green)\": green_hog_features,\n",
    "    \"HOG Multi-Scale (Blue)\": blue_hog_features,\n",
    "    \"HOG Multi-Scale (Gray)\": gray_hog_features,\n",
    "    \"HOG Multi-Scale (Hue)\": hue_hog_features,\n",
    "    \"HOG Multi-Scale (Saturation)\": saturation_hog_features,\n",
    "    \"HOG Multi-Scale (Brightness)\": brightness_hog_features,\n",
    "    # \"RGB + HSV\": RGB_HSV_features,\n",
    "    \"HOG Multi-Scale (RGB)\": hog_RGB_features,\n",
    "    \"HOG Multi-Scale (Gray+HSV)\": hog_gray_HSV_features,\n",
    "    \"RGB + HSV + ResNet\": RGB_HSV_ResNet_features,\n",
    "    \"RGB + HSV + ResNet + HOG (Gray)\": RGB_HSV_ResNet_GrayHOG_features,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_PCA(X_list, n_components=2):\n",
    "    pca_list = []\n",
    "    xpca_list = []\n",
    "    for X in X_list:\n",
    "        pca = PCA(n_components=n_components).fit(X)\n",
    "        X_pca = pca.transform(X)\n",
    "        pca_list.append(pca)\n",
    "        xpca_list.append(X_pca)\n",
    "    return pca_list, xpca_list\n",
    "\n",
    "def plot_PCA(X_list, labels, n_components=2):\n",
    "    pca_list, xpca_list = get_PCA(X_list, n_components=n_components)\n",
    "    plt.figure(figsize=(15,5))\n",
    "    #colors = ['b-', 'm-', 'k-']\n",
    "    for i in range(len(X_list)):\n",
    "        # plt.plot(np.cumsum(pca_list[i].explained_variance_ratio_), colors[i], label=labels[i])\n",
    "        plt.plot(np.cumsum(pca_list[i].explained_variance_ratio_), label=labels[i])\n",
    "    plt.yticks(np.linspace(0, 1, 8))\n",
    "    plt.xlabel('Number of components')\n",
    "    plt.ylabel('Explained Variances')\n",
    "    plt.legend(bbox_to_anchor=(1.02, 1), loc='upper left', borderaxespad=0)\n",
    "    #plt.savefig(r\"c:\\Users\\grays\\Repositories\\281 Computer Vision\\w281-final\\plots\\ResNet_EffNet_PCA_explained_200.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_feature_list = list(new_feature_combos_dict.values())\n",
    "new_feature_names = list(new_feature_combos_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = plot_PCA(X_list=new_feature_list, labels=new_feature_names, n_components=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py1torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
